{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import itertools\n",
    "from copy import deepcopy\n",
    "from collections import namedtuple, defaultdict\n",
    "\n",
    "from bs4 import BeautifulSoup as BS\n",
    "from tenacity import retry, stop_after_attempt\n",
    "import simplejson as json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = requests.Session()\n",
    "headers = {\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\",\n",
    "    \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "    \"Accept-Language\": \"zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7,ja;q=0.6\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"Referer\": \"https://www.google.com.tw/\",\n",
    "    \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) \"\\\n",
    "                  \"Chrome/69.0.3497.92 Safari/537.36\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def issue_page_to_url(issue, page_num):\n",
    "    url_format = 'https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/' \\\n",
    "                 'issue/{}?pageNum={}'\n",
    "    return url_format.format(issue, page_num)\n",
    "\n",
    "def get_website_url(suffix):\n",
    "    prefix = 'https://www.cambridge.org{}'\n",
    "    return prefix.format(suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(stop=stop_after_attempt(3))\n",
    "def get_bsObj(url):\n",
    "    req = session.get(url, headers=headers)\n",
    "    bsObj = BS(req.text, \"html.parser\")\n",
    "    return bsObj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_references_from_url(url):\n",
    "    \n",
    "    req = session.get(url, headers=headers)\n",
    "    art_bsObj = BS(req.text, \"html.parser\")\n",
    "    \n",
    "    reference_infos = []\n",
    "    for ref in art_bsObj.find_all('span', class_='mixed-citation journal'):\n",
    "        authors = []\n",
    "        for name in ref.find_all('span', class_='name'):\n",
    "            surname = name.find('span', class_='surname')\n",
    "            surname = surname.text if surname else None\n",
    "            \n",
    "            given_names = name.find('span', class_='given-names')\n",
    "            given_names = given_names.text if given_names else None\n",
    "            \n",
    "            detailed_name = DetailedName(surname=surname, given_names=given_names)\n",
    "            authors.append(detailed_name)\n",
    "\n",
    "        year = ref.find('span', class_='year')\n",
    "        year = year.text if year else None\n",
    "        \n",
    "        title = ref.find('span', class_='article-title')\n",
    "        title = title.text if title else None\n",
    "        \n",
    "        source = ref.find('span', class_='source')\n",
    "        source = source.text if source else None\n",
    "\n",
    "        reference_info = ReferenceInfo(authors=authors,\n",
    "                                       year=year,\n",
    "                                       title=title,\n",
    "                                       source=source)\n",
    "        reference_infos.append(reference_info)\n",
    "    return reference_infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ArticleInfo = namedtuple('ArticleInfo', ['authors', 'title', 'url', 'article_type', 'references'])\n",
    "AuthorInfo = namedtuple('Author', ['author_name', 'url'])\n",
    "DetailedName = namedtuple('DetailedName', ['surname', 'given_names'])\n",
    "ReferenceInfo = namedtuple('ReferenceInfo', ['authors', 'year', 'title', 'source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_infos_from_issue(issue):\n",
    "    article_infos = []\n",
    "    for page_num in itertools.count(start=1):\n",
    "\n",
    "        page = get_bsObj(issue_page_to_url(issue, page_num))\n",
    "\n",
    "        heading_banner = page.find('h4', class_='journal-article-listing-type heading_12 margin-bottom')\n",
    "        if not heading_banner:\n",
    "            break\n",
    "        assert heading_banner['class'] == ['journal-article-listing-type', 'heading_12', 'margin-bottom']\n",
    "\n",
    "        now_type = heading_banner.text\n",
    "        print('\\n', now_type, end='\\n\\n')\n",
    "\n",
    "        for idx, item in enumerate(heading_banner.find_next_siblings()):\n",
    "            link = item.find('a', class_='part-link')\n",
    "            authors = item.find('li', class_='author')\n",
    "            if link and authors:\n",
    "                author_infos = []\n",
    "                for author_objs in authors.find_all('a'):\n",
    "                    author_name = author_objs.text\n",
    "                    author_url = author_objs['href']\n",
    "                    author_infos.append(AuthorInfo(author_name=author_name, url=author_url))\n",
    "\n",
    "                title = link.text.strip()\n",
    "                url = get_website_url(link['href'])\n",
    "                references = get_references_from_url(url)\n",
    "                info = ArticleInfo(authors=author_infos,\n",
    "                                   title=title,\n",
    "                                   url=url,\n",
    "                                   article_type=now_type,\n",
    "                                   references=references)\n",
    "                article_infos.append(info)\n",
    "\n",
    "                print(title)\n",
    "\n",
    "            else:\n",
    "                if item['class'] != ['journal-article-listing-type', 'heading_12', 'margin-bottom']:\n",
    "                    continue\n",
    "                now_type = item.text\n",
    "                print('\\n', now_type, end='\\n\\n')\n",
    "    return article_infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_list_to_trees(articles, head_types=['Target Article'], children_types=['Open Peer Commentary']):\n",
    "    head_to_children = dict()\n",
    "    \n",
    "    head_article = None\n",
    "    \n",
    "    article_types = set()\n",
    "    for article in articles:\n",
    "        \n",
    "        article_types.add(article['article_type'])\n",
    "\n",
    "        if article['article_type'] in head_types:\n",
    "\n",
    "            _article = deepcopy(article)\n",
    "            title = _article.pop('title', None)\n",
    "            head_article = title\n",
    "            \n",
    "\n",
    "        elif article['article_type'] in children_types:\n",
    "            \n",
    "            if head_article is None:\n",
    "                break\n",
    "\n",
    "            if title not in head_to_children:\n",
    "                head_to_children[title] = _article\n",
    "                head_to_children[title]['commentaries'] = []\n",
    "                \n",
    "\n",
    "            head_to_children[head_article]['commentaries'].append(article)\n",
    "        \n",
    "        else:\n",
    "            head_article = None\n",
    "\n",
    "    if not all(children_lists['commentaries'] for head, children_lists in head_to_children.items()):\n",
    "        print([head for head, children_lists in head_to_children.items() if children_lists['commentaries']])\n",
    "        print([head for head, children_lists in head_to_children.items() if not children_lists['commentaries']])\n",
    "        assert False\n",
    "    \n",
    "    return head_to_children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_issues = get_bsObj(\"https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/all-issues\")\n",
    "year_to_issues = defaultdict(list)\n",
    "for bsObj in all_issues.find_all('span', text=re.compile(\"[0-9]{4} \\- Volume \")):\n",
    "    year = int(bsObj.text[:4])\n",
    "    block = bsObj.parent.parent.parent.find('ul', class_='accordion level fourth')\n",
    "    for single_issue in block.find_all('a', class_='row'):\n",
    "        year_to_issues[year].append(single_issue['href'].split('/')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('articles/articles.json'):\n",
    "    year_to_articles = {}\n",
    "else:\n",
    "    with open('articles/articles.json', 'r') as f:\n",
    "        year_to_articles = json.load(f)\n",
    "\n",
    "for year, issues in year_to_issues.items():\n",
    "    if str(year) in year_to_articles:\n",
    "        continue\n",
    "    else:\n",
    "        article_infos_whole_year = []\n",
    "        for issue in issues:\n",
    "            print('Downloading issue id: %s of year %d' % (issue, year))\n",
    "            article_infos = get_article_infos_from_issue(issue)\n",
    "            article_infos_whole_year.extend(article_infos)\n",
    "        year_to_articles[str(year)] = article_infos_whole_year\n",
    "\n",
    "with open('articles/articles.json', 'w') as f:\n",
    "    json.dump(year_to_articles, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "year_to_selection = {}\n",
    "\n",
    "def check_main_article(article_type):\n",
    "    return article_type in ['Main Articles',\n",
    "                            'Main Article',\n",
    "                            'Target Article',\n",
    "                            'Target Articles',\n",
    "                            'Research Article',\n",
    "                            'Target article']\n",
    "\n",
    "def check_commentary(article_type):\n",
    "    return article_type in ['Open Peer Commentary',\n",
    "                            'open peer commentary',\n",
    "                            'Continuing Commentary']\n",
    "\n",
    "with open('article_types.csv', 'w+') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['year', 'All Types', 'Main Articles', 'Commentary'])\n",
    "\n",
    "    for year, articles in year_to_articles.items():\n",
    "        article_types = set(article['article_type'] for article in articles)\n",
    "        \n",
    "        main_articles = [article_type for article_type in article_types if check_main_article(article_type)]\n",
    "        commentaries = [article_type for article_type in article_types if check_commentary(article_type)]\n",
    "\n",
    "        year_to_selection[year] = (main_articles, commentaries)\n",
    "        \n",
    "        writer.writerow([year, ', '.join(article_types), ', '.join(main_articles), ', '.join(commentaries)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year, articles in year_to_articles.items():\n",
    "    trees = transform_list_to_trees(articles, *year_to_selection[year])\n",
    "    with open('articles/{}_articles.json'.format(year), 'w') as f:\n",
    "        json.dump(trees, f, ensure_ascii=False, indent=4)   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
